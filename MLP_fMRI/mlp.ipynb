{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZKUpjEb6wTZ"
      },
      "outputs": [],
      "source": [
        "!pip install nibabel\n",
        "!pip install torchio\n",
        "!pip install onnx\n",
        "!pip install onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets,transforms, models\n",
        "import torchvision.transforms.functional as TF\n",
        "import nibabel as nib\n",
        "from pathlib import Path\n",
        "import onnxruntime\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from scipy import stats\n",
        "import onnx"
      ],
      "metadata": {
        "id": "B6KLtDsD7BOU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to match fMRI ICA 100 components\n",
        "batch_size = 100"
      ],
      "metadata": {
        "id": "CiBuxhgm7NMB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultilayerPerceptron2(nn.Module):\n",
        "  # whatever op layer is becomes num ip for next hidden layer\n",
        "  def __init__(self, input_size=45*54*45, output_size=58):\n",
        "    super().__init__()\n",
        "    N = 200\n",
        "    self.d1 = nn.Linear(input_size, N)\n",
        "    self.d2 = nn.Linear(N, N)\n",
        "    self.d3 = nn.Linear(N, N)\n",
        "    self.d4 = nn.Linear(N, N)\n",
        "    self.d5 = nn.Linear(N, output_size)\n",
        "    self.dropout = nn.Dropout(0.66)\n",
        "    self.flat = nn.Flatten()\n",
        "\n",
        "\n",
        "  def forward(self,X):\n",
        "\n",
        "    X = self.flat(X) #X.view(-1,45*54*45)\n",
        "    X = F.relu(self.d1(X))\n",
        "    X = F.relu(self.d3(X))\n",
        "    X = self.dropout(X)\n",
        "    X = F.relu(self.d4(X))\n",
        "    X = self.d5(X)\n",
        "    #X = torch.squeeze(X)\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "TnJ0C5rk7Nvd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "print(f'Device is {device}')\n",
        "model_final =  torch.load('/content/drive/MyDrive/Colab Notebooks/model_mlp_final.pth', map_location=device)\n",
        "\n",
        "model = MultilayerPerceptron2()\n",
        "\n",
        "\n",
        "model.load_state_dict(model_final['model'])\n",
        "model = model.to(device)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw5SBCfh7NzE",
        "outputId": "c4a1696a-2109-4f78-e15f-08300250be3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultilayerPerceptron2(\n",
              "  (d1): Linear(in_features=109350, out_features=200, bias=True)\n",
              "  (d2): Linear(in_features=200, out_features=200, bias=True)\n",
              "  (d3): Linear(in_features=200, out_features=200, bias=True)\n",
              "  (d4): Linear(in_features=200, out_features=200, bias=True)\n",
              "  (d5): Linear(in_features=200, out_features=58, bias=True)\n",
              "  (dropout): Dropout(p=0.66, inplace=False)\n",
              "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_onnx = str(Path(\"mlp_model.onnx\"))\n",
        "input_volume = torch.randn(1, 1, 45, 54, 45)\n",
        "\n",
        "# Export an ONNX model.\n",
        "with torch.no_grad():\n",
        "    torch.onnx.export(\n",
        "        model=model,\n",
        "        args=(input_volume),\n",
        "        f=output_onnx,\n",
        "        opset_version=13,\n",
        "        verbose=True,\n",
        "        input_names=[\"image\"],\n",
        "        output_names=[ \"score\"],\n",
        "    )"
      ],
      "metadata": {
        "id": "cKJt9Tmx7N2t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "volPath = \"/content/drive/MyDrive/Colab Notebooks/rgb_images/\"\n",
        "niii_mg = nib.load(volPath).get_fdata()"
      ],
      "metadata": {
        "id": "7DmViUrjkQvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load the image\n",
        "img = Image.open(\"/content/drive/MyDrive/Colab Notebooks/rgb_images/image_00001.png\")\n",
        "\n",
        "print(img)\n",
        "\n",
        "# Convert the image to a tensor\n",
        "tensor = transforms.ToTensor()(img)\n",
        "\n",
        "# Print the size of the tensor\n",
        "print(tensor.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z4kAI4XphdK",
        "outputId": "dda237ce-449a-45d7-ed0b-2210b3c11c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=45x45 at 0x7F3CBEE635E0>\n",
            "torch.Size([3, 45, 45])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define the new size\n",
        "new_size = (45, 54, 45)\n",
        "\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(new_size[1:]),  # Resize to (54, 45)\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def get_arr_from_image(img_path):\n",
        "    img = Image.open(img_path)  # Convert to grayscale\n",
        "    arr = preprocess(img).unsqueeze(0).cpu().detach().numpy()\n",
        "    return arr\n",
        "\n",
        "# Example usage:\n",
        "img_path = \"/content/drive/MyDrive/Colab Notebooks/rgb_images/image_00001.png\"\n",
        "result_array = get_arr_from_image(img_path)\n",
        "print(result_array.shape)  # It should print (1, 45, 54, 45)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeE_LDKFlT72",
        "outputId": "a6f2bddb-dc14-4436-8b56-41c2bb80b972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3, 54, 45)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l9n5rDPQ7N53"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}